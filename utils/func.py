import requests
from lxml import etree
from prettytable import PrettyTable
import re
from database.user_database import UserDatabase
from bs4 import BeautifulSoup
import requests
user_db = UserDatabase()

def format_large_numbers(number):
    if number >= 1000:
        formatted_number = "{:.1f}k".format(number / 1000)
    else:
        formatted_number = str(number)
    return formatted_number

def organize_data(data_list):
    formatted_data = []
    i = 0
    for item in data_list:
        i +=1    
        name, link, category, date, members = item
        if "channel" in category:
            category = "ğŸ“¢"
        else:
            category = "ğŸ‘¥"
        formatted_entry = f"{i},{category}[{name} -{members}]({link})"
        formatted_data.append(formatted_entry)
    result = '\n'.join(formatted_data)
    return result


def parse_user_data(user_data_list):
    i = 0
    table = PrettyTable()
    table.field_names = ["id", "ç”¨æˆ·å", "ç”¨æˆ·ID"]
    for user_tuple in user_data_list:
        user_id, username, is_blacklisted = user_tuple
        if username is None:
            continue
        i += 1
        table.add_row([i, f"@{username}", user_id])
    return str(table)

def parse_query_result_to_dict(query_result):
    result_dicts = []
    for row in query_result:
        user_id, username, is_blacklisted = row
        if username is None:
            continue
        data_dict = {
            "id": user_id,
            "username": username,
            "is_blacklisted": bool(is_blacklisted)
        }
        result_dicts.append(data_dict)
    return result_dicts


def extract_members_prefix(text):
    """
    ä»æ–‡æœ¬ä¸­æå–åŒ¹é… "members" å‰é¢çš„æ‰€æœ‰æ•°æ®.
    Args:
        text (str): è¾“å…¥æ–‡æœ¬.

    Returns:
        str or None: å¦‚æœæ‰¾åˆ°åŒ¹é…çš„æ•°æ®,è¿”å›æå–çš„æ•°æ®,å¦åˆ™è¿”å› None.
    """
    match = re.search(r'(.+?)\s+members', text)
    if match:
        result = match.group(1).replace(" ", "")
        return result
    else:
        return None

def extract_all_numbers_as_string(text):
    """
    ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰æ•°å­—å¹¶è¿æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸².
    Args:
        text (str): è¾“å…¥æ–‡æœ¬.

    Returns:
        str or None: å¦‚æœæ‰¾åˆ°æ•°å­—,è¿”å›è¿æ¥åçš„å­—ç¬¦ä¸²,å¦åˆ™è¿”å› None.
    """
    pattern = r'\d+'
    matches = re.findall(pattern, text)
    if matches:
        return ''.join(matches)
    else:
        return None

def get_telegram_info(url):
    """
    å‘é€è¯·æ±‚å¹¶è§£æ Telegram é¡µé¢ä¿¡æ¯.
    Args:
        url (str): è¦è¯·æ±‚çš„ URL.

    Returns:
        None
    """
    try:
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            response = etree.HTML(response.text)
            anonymous_groups = response.xpath('//a[@class="tgme_username_link"]')
            if anonymous_groups:
                return

            title = response.xpath('//span[@dir="auto"]/text()')[0]
            members_text = response.xpath('//div[@class="tgme_page_extra"]/text()')[0]
            
            if "subscribers" in members_text:
                group_type = "channel"
                members_count = extract_all_numbers_as_string(members_text)
            else:
                group_type = "supergroup"
                members_count = extract_members_prefix(members_text)
            
            return url, title, members_count, group_type
    except requests.exceptions.RequestException as e:
        return False
    


def remove_span_tags_and_keep_text(input_string):
    soup = BeautifulSoup(input_string, 'html.parser')
    text_without_span = soup.get_text()
    return text_without_span

def get_data_for_kw_and_page(kw, page=1):
    base_url = "https://tgscan.xyz/api/search/query"
    params = {"kw": kw, "p": page, "t": ""}
    results = []
    headers = {"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36"}
    with requests.Session() as session:
        response = session.get(base_url, params=params, headers=headers)
        data = response.json()["doc"]
        totalPage = response.json()["totalPage"]

        for idata in data:
            if "MESSAGE" in idata["type"]:

                idata["memberCnt"] = "æ¶ˆæ¯å†…å®¹"
                idata["highlighting"]["name"] = idata["highlighting"]["title"]
                #æŠ¥é”™ä¿®å¤æœªæµ‹è¯•
                idata["link"] = user_db.get_url(idata["chatId"])[1]+"/"+str(idata["offset"])
            result_dict = {
                "type": idata["type"],
                "memberCnt": idata["memberCnt"],
                "name": remove_span_tags_and_keep_text(idata["highlighting"]["name"]),
                "link": idata["link"]
            }
            results.append(result_dict)
        results.append({"totalPage": totalPage})
    return results

def remove_special_characters(input_string):
    """
    è¿‡æ»¤å­—ç¬¦ä¸²ä¸­çš„ç‰¹æ®Šå­—ç¬¦.

    å‚æ•°:
    - input_string: è¾“å…¥å­—ç¬¦ä¸²

    è¿”å›å€¼:
    - è¿‡æ»¤åçš„å­—ç¬¦ä¸²
    """
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ç‰¹æ®Šå­—ç¬¦å¹¶æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²
    pattern = r'[!@#$%^&*()_+{}\[\]:;<>,.?~\\\/|"\']'
    filtered_string = re.sub(pattern, '', input_string)
    
    return filtered_string
def dict_to_markdown_links(dictionary_list):
    markdown_links = []
    for dictionary in dictionary_list:
        if "totalPage" in dictionary:
            continue
        if "MESSAGE" in dictionary["type"]:
            memberCnt = "æ¶ˆæ¯å†…å®¹"
        else:
            memberCnt = format_large_numbers(dictionary["memberCnt"])
        if "CHANNEL" in dictionary["type"]:
            category = "ğŸ“¢"
        elif "MESSAGE" in dictionary["type"]:
            category = "ğŸ’¬"
        else:
            category = "ğŸ‘¥"
        formatted_link = f"{category}[{remove_special_characters(dictionary['name'])} -{memberCnt}]({dictionary['link']})"
        markdown_links.append(formatted_link)
    markdown_links.append(f"\n{dictionary['totalPage']}")
    result = "\n".join(markdown_links)
    return result,True if dictionary['totalPage'] >= 1 else False,dictionary['totalPage']
